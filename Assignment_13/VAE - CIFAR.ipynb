{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchbearer import Callback\n",
    "\n",
    "# import cv2\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Albumentations for augmentations\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !install_package_python310.sh add albumentations\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Albumentations for augmentations\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# cv2.setNumThreads(0)\n",
    "# cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "class Cifar10SearchDataset(torchvision.datasets.CIFAR10):\n",
    "    def __init__(self, root=\"~/data/cifar10\", train=True, download=True, transform=None):\n",
    "      super().__init__(root=root, train=train, download=download, transform=transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index], self.targets[index]\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "      # A.RandomCrop(width=16, height=16),\n",
    "      A.HorizontalFlip(p=0.5),\n",
    "      A.CoarseDropout(max_holes = 1, max_height=16, max_width=16, min_holes = 1, min_height=16, min_width=16, fill_value=(0.5, 0.5, 0.5), mask_fill_value = None),\n",
    "      A.ShiftScaleRotate(),\n",
    "      # A.RandomBrightnessContrast(p=0.2),\n",
    "      A.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "      ToTensorV2(),\n",
    "    ],\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Normalize((0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)),\n",
    "      ToTensorV2(),\n",
    "], p=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "class args:\n",
    "    def __init__(self, device=\"cpu\", use_cuda=False) -> None:\n",
    "        self.batch_size = 64\n",
    "        self.device = device\n",
    "        self.use_cuda = use_cuda\n",
    "        self.kwargs = {\"num_workers\": 1, \"pin_memory\": True} if self.use_cuda else {}\n",
    "\n",
    "trainset = Cifar10SearchDataset(\n",
    "    root=\"./data\", train=True, download=True, transform=train_transforms\n",
    ")\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=args().batch_size, shuffle=True, **args().kwargs\n",
    ")\n",
    "\n",
    "testset = Cifar10SearchDataset(\n",
    "    root=\"./data\", train=False, download=True, transform=test_transforms\n",
    ")\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=args().batch_size, shuffle=True, **args().kwargs\n",
    ")\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_value = 0.0\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.convblock_0 = nn.Sequential(\n",
    "                       nn.Conv2d(in_channels=3,out_channels=16,kernel_size=(3,3),dilation=1,stride=1,padding=1,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(16),\n",
    "                       nn.Dropout(dropout_value), # Input - 32x32x3 | Output - 32X32X16 | RF=3\n",
    "\n",
    "                       nn.Conv2d(in_channels=16,out_channels=32,kernel_size=(3,3),dilation=1,stride=1,padding=1,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(32),\n",
    "                       nn.Dropout(dropout_value), # Input - 32X32X16 | Output - 32X32x32 |RF=5\n",
    "\n",
    "                       nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(3,3),dilation=1,stride=1,padding=1,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(32),\n",
    "                       nn.Dropout(dropout_value), # Input - 32X32X32 | Output - 32X32X64 |RF= 7\n",
    "\n",
    "                       nn.Conv2d(in_channels=32,out_channels=32,kernel_size=(3,3),dilation=1,stride=1,padding=1,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(32),\n",
    "                       nn.Dropout(dropout_value), # Input - 32X32X64 | Output - 32X32X64 |RF= 9\n",
    "                      )\n",
    "        \n",
    "        # depthwise seperable Convolution 1\n",
    "        self.convblock_1 = nn.Sequential(\n",
    "        \n",
    "                       nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(3,3),stride=(2,2),dilation=1,padding=1,bias=False,),# maxpool added after RF >11\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(64),\n",
    "                       nn.Dropout(dropout_value), # Input - 32X32X64 | Output - 16X16X64 |RF=11\n",
    "\n",
    "                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),dilation=1,padding=1,bias=False,),\n",
    "                       # Input - 16X16X64 | Output - 16X16X64 | RF=15\n",
    "                       nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),\n",
    "                       # Input - 16X16X64 | Output - 16X16X64 | RF=15\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(128), \n",
    "                       nn.Dropout(dropout_value), # 16X16X64 | RF=21                                       \n",
    "                       # pointwise   \n",
    "\n",
    "                       nn.Conv2d(in_channels=128,out_channels=128,groups=128,kernel_size=(3,3),dilation=1,stride=(1,1),padding=1,bias=False,),\n",
    "                       # Input - 16X16X64 | Output - 16X16X64 | RF=29\n",
    "                       nn.Conv2d(in_channels=128,out_channels=64,kernel_size=(1,1),padding=0,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(64),   \n",
    "                       nn.Dropout(dropout_value), \n",
    "                       # Input - 16X16X64 | Output - 16X16X32 | RF=29\n",
    "                       \n",
    "                      #nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),stride=(1,1),dilation=2,padding=1,bias=False,),\n",
    "                      # #  nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),\n",
    "                      # #  nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),padding=0,bias=False,),\n",
    "                      #  nn.ReLU(),\n",
    "                      #  nn.BatchNorm2d(64),   \n",
    "                      #  nn.Dropout(dropout_value) , # 16X16X64 | RF=29                                                         \n",
    "                       )\n",
    "        # depthwise seperable Convolution 2\n",
    "        self.convblock_2 = nn.Sequential(\n",
    "        \n",
    "                       nn.Conv2d(in_channels=64,out_channels=32,kernel_size=(3,3),stride=(2,2),dilation=1,padding=1,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(32),   \n",
    "                       nn.Dropout(dropout_value), \n",
    "                      # # Input - 16X16X32 | Output - 8X8X32 | RF=37\n",
    "\n",
    "                       nn.Conv2d(in_channels=32,out_channels=32,groups=32,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),\n",
    "                       # # Input - 8X8X32 | Output - 8X8X32 | RF=45\n",
    "                       nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),\n",
    "                       # # Input - 8X8X32 | Output - 8X8X64 | RF=45\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(64),  \n",
    "                       nn.Dropout(dropout_value),\n",
    "                      # pointwise   \n",
    "\n",
    "                       nn.Conv2d(in_channels=64,out_channels=64,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),\n",
    "                       # # Input - 8X8X64 | Output - 8X8X128 | RF=53\n",
    "                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(64),  # pointwise \n",
    "                       nn.Dropout(dropout_value) \n",
    "                       # # Input - 8X8X64 | Output - 8X8X64 | RF=53\n",
    "\n",
    "                      )\n",
    "        # depthwise seperable Convolution 2\n",
    "        self.convblock_3 = nn.Sequential(\n",
    "        \n",
    "                       #Maxpooling\n",
    "                       nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(3,3),dilation=1,stride=(2,2),padding=1,bias=False),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(64),  \n",
    "                       nn.Dropout(dropout_value),\n",
    "                      # # Input - 8X8X64 | Output - 4X4X64 | RF=69\n",
    "\n",
    "                       nn.Conv2d(in_channels=64,out_channels=128,groups=64,kernel_size=(3,3),stride=(1,1),padding=1,bias=False,),\n",
    "                       # # Input - 4X4X64 | Output - 4X4X128 | RF=85\n",
    "                       nn.Conv2d(in_channels=128,out_channels=192,kernel_size=(1,1),stride=(1,1),padding=0,bias=False,),\n",
    "                       nn.ReLU(),\n",
    "                       nn.BatchNorm2d(192),\n",
    "                       nn.Dropout(dropout_value),\n",
    "                      #  # Input - 4X4X128 | Output - 4X4X192 | RF=85\n",
    "                     \n",
    "#                        nn.Conv2d(in_channels=64,out_channels=32,kernel_size=(3,3),dilation=2,stride=(1,1),padding=2,bias=False),\n",
    "#                        nn.ReLU(),\n",
    "#                        nn.BatchNorm2d(32),  \n",
    "#                        nn.Dropout(dropout_value),  \n",
    "#                        #  # Input - 4X4X128 | Output - 4X4X64 | RF=117\n",
    "\n",
    "#                        nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False)\n",
    "#                        # Input - 4X4X32 | Output - 4X4X10 | RF=117\n",
    "\n",
    "                       )\n",
    "        # 4X4X10 | RF=121\n",
    "        self.gap = nn.Sequential(nn.AvgPool2d(kernel_size=1))\n",
    "        self.fc = nn.Linear(in_features = 10, out_features = 20)\n",
    "        \n",
    "        self.fc1 = nn.Linear(3072, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc30 = nn.Linear(256, 20)\n",
    "        self.fc31 = nn.Linear(256, 20)\n",
    "        self.fc32 = nn.Linear(256, 20)\n",
    "        self.fc4 = nn.Linear(20, 256)\n",
    "        self.fc5 = nn.Linear(256, 1024)\n",
    "        self.fc6 = nn.Linear(1024, 3072)\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.convblock_0(x)\n",
    "        x = self.convblock_1(x)\n",
    "        x = self.convblock_2(x)\n",
    "        x = self.convblock_3(x)\n",
    "        x = self.gap(x)\n",
    "#         print(f\"gap output {x.shape}\")\n",
    "        x = x.view(-1,3072)\n",
    "#         print(f\"reshaped output {x.shape}\")\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc30(x)\n",
    "#         x = self.fc4(x)\n",
    "#         x = self.fc5(x)\n",
    "#         x = self.fc6(x)\n",
    "#         x = x.view(-1, 3072)\n",
    "#         print(f\"1st {torch.isnan(sum(x))}\")        \n",
    "        y = torch.nn.functional.one_hot(y, num_classes = 10) # One hot encoding of the label\n",
    "        y = y.type(torch.cuda.FloatTensor)\n",
    "        y = self.fc(y)\n",
    "        y = self.fc4(y)\n",
    "        y = self.fc5(y)\n",
    "        y = self.fc6(y)\n",
    "#         print(f\"x.view(-1, 3072) shape {x.view(-1, 3072).shape}\")\n",
    "#         print(f\"y shape {y.shape}\")\n",
    "        x = torch.add(x.view(-1, 3072), y)\n",
    "#         print(f\"x added view(-1, 3072) shape {x.view(-1, 3072).shape}\")\n",
    "        mu, logvar = self.encode(x.view(-1, 3072))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "def bce_loss(y_pred, y_true):\n",
    "    BCE = F.binary_cross_entropy(y_pred, y_true.view(-1, 1024), size_average=False)\n",
    "    return BCE\n",
    "\n",
    "\n",
    "class AddKLDLoss(Callback):\n",
    "    def on_criterion(self, state):\n",
    "        super().on_criterion(state)\n",
    "        KLD = self.KLD_Loss(state['mu'], state['logvar'])\n",
    "        state[torchbearer.LOSS] = state[torchbearer.LOSS] + KLD\n",
    "\n",
    "    def KLD_Loss(self, mu, logvar):\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return KLD\n",
    "\n",
    "\n",
    "class SaveReconstruction(Callback):\n",
    "    def __init__(self, num_images=8, folder='results/'):\n",
    "        super().__init__()\n",
    "        self.num_images = num_images\n",
    "        self.folder = folder\n",
    "\n",
    "    def on_step_validation(self, state):\n",
    "        super().on_step_validation(state)\n",
    "        if state[torchbearer.BATCH] == 0:\n",
    "            data = state[torchbearer.X]\n",
    "            recon_batch = state[torchbearer.Y_PRED]\n",
    "            comparison = torch.cat([data[:self.num_images],\n",
    "                                    recon_batch.view(128, 1, 28, 28)[:self.num_images]])\n",
    "            save_image(comparison.cpu(),\n",
    "                       str(self.folder) + 'reconstruction_' + str(state[torchbearer.EPOCH]) + '.png', nrow=self.num_images)\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "loss = bce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 3072), size_average=False)\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, traingen, optimizer, epoch, train_losses):\n",
    "    model.train()\n",
    "    pbar = tqdm(traingen)\n",
    "    for batch_idx, (images, _) in enumerate(pbar):\n",
    "        x = images\n",
    "        x = x.to(device)\n",
    "        y = _\n",
    "        y = y.to(device)\n",
    "        model = model.to(device)\n",
    "        output_1, mu_1, logvar_1 = model(x, y)\n",
    "        loss = loss_function(output_1, x, mu_1, logvar_1)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(desc= f'Loss={loss.item()} epoch={epoch}')\n",
    "        train_losses.append(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, device, testgen, optimizer, epoch, test_losses):\n",
    "    model.eval()\n",
    "    pbar = tqdm(testgen)\n",
    "    test_loss = 0\n",
    "    additional_train_loader_dataset = 10000\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(pbar):\n",
    "            x = images\n",
    "            x = x.to(device)\n",
    "            y = _\n",
    "            y = y.to(device)\n",
    "            model = model.to(device)\n",
    "            output_1, mu_1, logvar_1 = model(x, y)\n",
    "            output_1 = torch.nan_to_num(output_1)\n",
    "            loss = loss_function(output_1, x, mu_1, logvar_1)\n",
    "            pbar.set_description(desc= f'Loss={loss.item()} epoch={epoch}')\n",
    "        test_loss = loss.item() / additional_train_loader_dataset\n",
    "        test_losses.append(test_loss)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}\\n'.format(test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (convblock_0): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.0, inplace=False)\n",
       "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.0, inplace=False)\n",
       "    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (convblock_1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "    (10): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (convblock_2): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "    (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "    (10): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (convblock_3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.0, inplace=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "    (5): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (gap): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (fc): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc1): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc30): Linear(in_features=256, out_features=20, bias=True)\n",
       "  (fc31): Linear(in_features=256, out_features=20, bias=True)\n",
       "  (fc32): Linear(in_features=256, out_features=20, bias=True)\n",
       "  (fc4): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=1024, bias=True)\n",
       "  (fc6): Linear(in_features=1024, out_features=3072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "model = VAE()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1,  momentum=0.9)\n",
    "model = model.to(device)\n",
    "test_losses = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Loss=nan epoch=0: 100%|██████████| 782/782 [00:17<00:00, 43.65it/s]        \n",
      "Loss=nan epoch=0: 100%|██████████| 157/157 [00:01<00:00, 116.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: nan\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=nan epoch=1: 100%|██████████| 782/782 [00:17<00:00, 45.46it/s]\n",
      "Loss=nan epoch=1: 100%|██████████| 157/157 [00:01<00:00, 117.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: nan\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss=nan epoch=2:  16%|█▌        | 125/782 [00:02<00:14, 45.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     test_model(model, device, testloader, optimizer, epoch, test_losses)\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, traingen, optimizer, epoch, train_losses)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(traingen)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mCifar10SearchDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m image, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/albumentations/core/composition.py:205\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    202\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 205\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    208\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_fullname() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because its\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m params depend on targets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dependence\u001b[38;5;241m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_dependencies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/albumentations/augmentations/dropout/coarse_dropout.py:96\u001b[0m, in \u001b[0;36mCoarseDropout.apply\u001b[0;34m(self, img, fill_value, holes, **params)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     91\u001b[0m     img: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m     95\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcutout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dsw/snapshots/1fbd09c1-6300-44c5-b2e5-8ccbc2ca8da3/python310/lib/python3.10/site-packages/albumentations/augmentations/dropout/functional.py:26\u001b[0m, in \u001b[0;36mcutout\u001b[0;34m(img, holes, fill_value)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcutout\u001b[39m(\n\u001b[1;32m     23\u001b[0m     img: np\u001b[38;5;241m.\u001b[39mndarray, holes: Iterable[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]], fill_value: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Make a copy of the input image since we don't want to modify it directly\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x1, y1, x2, y2 \u001b[38;5;129;01min\u001b[39;00m holes:\n\u001b[1;32m     28\u001b[0m         img[y1:y2, x1:x2] \u001b[38;5;241m=\u001b[39m fill_value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train(model, device, trainloader, optimizer, epoch, train_losses)\n",
    "    test_model(model, device, testloader, optimizer, epoch, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(testloader)\n",
    "for batch_idx, (images, _) in enumerate(pbar):\n",
    "    x = images\n",
    "    x = x.to(device)\n",
    "    y = _\n",
    "    y = y.to(device)\n",
    "    model = model.to(device)\n",
    "    output_1, mu_1, logvar_1 = model(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test = output_1.reshape(-1,3,32,32)\n",
    "# plt.imshow(test[0].cpu().detach().numpy().squeeze(), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 10, 10\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(test), size=(1,)).item()\n",
    "    img = test[sample_idx].cpu().detach().squeeze()\n",
    "    img = img.permute(1, 2, 0)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "#     plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(testloader)\n",
    "for batch_idx, (images, _) in enumerate(pbar):\n",
    "    x = images\n",
    "    x = x.to(device)\n",
    "#     print(x.shape)\n",
    "    y = torch.randint(0, 9, (128, 1)).squeeze()\n",
    "    y = y.to(device)\n",
    "#     print(y.shape)\n",
    "    model = model.to(device)\n",
    "    output_1, mu_1, logvar_1 = model(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = output_1.reshape(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 10, 10\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(test_new), size=(1,)).item()\n",
    "    img = test_new[sample_idx].detach().numpy().squeeze()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "#     plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = classes[0]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(testloader)\n",
    "for batch_idx, (images, _) in enumerate(pbar):\n",
    "    x = images\n",
    "    x = x.to(device)\n",
    "    y = _\n",
    "    print(x.shape)\n",
    "#     y = classes.index(_)\n",
    "    print(y.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = torch.randint(len(test), size=(1,)).item()\n",
    "sample_idx\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(len(test)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[sample_idx].detach().numpy().squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Python 3.10 (General DS)",
   "language": "python",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
