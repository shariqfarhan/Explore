# Part 1 of the Assignment - Session 3 - Backpropagation and Architectural Basics

## Introduction

In the Attached Excel file, we have showcased the inner working of the backpropagation algorithm. This readme provides an overview of the outputs. For a given input and output neurons, and the weights given - we identify the loss function and try to reduce it.

## Impact of learning rate on learning

Learning rate = 0.1

<img width="441" alt="image" src="https://user-images.githubusercontent.com/57046534/212279999-78dfde06-280f-4800-a3fd-e7efe4f34375.png">

Learning rate = 0.2

<img width="429" alt="image" src="https://user-images.githubusercontent.com/57046534/212280090-6a656c55-a68c-40db-8cbb-ed6cc3cfeb14.png">

Learning rate = 0.5

<img width="424" alt="image" src="https://user-images.githubusercontent.com/57046534/212280353-8183af91-978f-490e-969e-6c1fd37b7ed0.png">


Learning rate = 0.8

<img width="440" alt="image" src="https://user-images.githubusercontent.com/57046534/212280392-36b1a48a-6a2c-46ae-986b-eec295b8f85f.png">


Learning rate = 1.0

<img width="441" alt="image" src="https://user-images.githubusercontent.com/57046534/212280431-4c936e17-b0df-4ee4-a4a4-f936463b2676.png">


Learning rate = 2.0

<img width="545" alt="image" src="https://user-images.githubusercontent.com/57046534/212280483-a8e4d1ec-7efe-4205-8bdd-5fb3fa457e49.png">

## Conclusions

We see that as the learning rates increase the learning happens quicker. 



